# -*- coding: utf-8 -*-
"""qa transformers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16MC8CYGYxcwhqmzoRUZ9tNky4xUCXJ9O
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

!apt install cargo -y

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/nlp-with-transformers/notebooks.git
# %cd notebooks
from install import *

!pip install farm-haystack[colab]

!pip install datasets

import datasets
from utils import *
setup_chapter()

from datasets import get_dataset_config_names

domains = get_dataset_config_names('subjqa')
domains

from datasets import load_dataset
subjqa = load_dataset("subjqa", name="electronics");

print(subjqa["train"]["answers"][1])

print(subjqa["train"])

dfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}
for split, df in dfs.items():
 print(f"Number of questions in {split}: {df['id'].nunique()}")

qa_cols = ["title", "question", "answers.text",
 "answers.answer_start", "context"]
sample_df = dfs["train"][qa_cols].sample(2, random_state=7)
sample_df

start_idx = sample_df["answers.answer_start"].iloc[0][0]
end_idx = start_idx + len(sample_df["answers.text"].iloc[0][0])
sample_df["context"].iloc[0][start_idx:end_idx]

counts = {}
question_types = ["What", "How", "Is", "Does", "Do", "Was", "Where", "Why"]
for q in question_types:
 counts[q] = dfs["train"]["question"].str.startswith(q).value_counts()[True]
pd.Series(counts).sort_values().plot.barh()
plt.title("Frequency of Question Types")
plt.show()

for question_type in ["How", "What", "Is"]:
 for question in (
  dfs["train"][dfs["train"].question.str.startswith(question_type)]
  .sample(n=3, random_state=42)['question']):
  print(question)

from transformers import AutoTokenizer

model_ckpt = "deepset/minilm-uncased-squad2"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

question = "How much music can this hold?"
context = """An MP3 is about 1 MB/minute, so about 6000 hours depending on \
file size."""
inputs = tokenizer(question, context, return_tensors="pt")

inputs.data

print(tokenizer.decode(inputs["input_ids"][0]))

import torch
from transformers import AutoModelForQuestionAnswering
model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)
with torch.no_grad():
 outputs = model(**inputs)
print(outputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits

print(f"Input IDs shape: {inputs.input_ids.size()}")
print(f"Start logits shape: {start_logits.size()}")
print(f"End logits shape: {end_logits.size()}")

import torch
start_idx = torch.argmax(start_logits)
end_idx = torch.argmax(end_logits) + 1
answer_span = inputs["input_ids"][0][start_idx:end_idx]
answer = tokenizer.decode(answer_span)
print(f"Question: {question}")
print(f"Answer: {answer}")

from transformers import pipeline
pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)
pipe(question=question, context=context, topk=3)

pipe(question="Why is there no data?", context=context,
handle_impossible_answer=True)

example = dfs["train"].iloc[0][["question", "context"]]
tokenized_example = tokenizer(example["question"], example["context"],
 return_overflowing_tokens=True, max_length=100,
 stride=25)

for idx, window in enumerate(tokenized_example["input_ids"]):
 print(f"Window #{idx} has {len(window)} tokens")

for window in tokenized_example["input_ids"]:
 print(f"{tokenizer.decode(window)} \n")

url = """https://artifacts.elastic.co/downloads/elasticsearch/\
elasticsearch-7.9.2-linux-x86_64.tar.gz"""
!wget -nc -q {url}
!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz

import os
from subprocess import Popen, PIPE, STDOUT
# Run Elasticsearch as a background process
!chown -R daemon:daemon elasticsearch-7.9.2
es_server = Popen(args=['elasticsearch-7.9.2/bin/elasticsearch'],
 stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))
# Wait until Elasticsearch has started
!sleep 30

!curl -X GET "localhost:9200/?pretty"

!pip install --upgrade pillow

import haystack
print(haystack.__version__)

!pip uninstall farm-haystack -y
!pip install "farm-haystack[all]" --upgrade

from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore
# Return the document embedding for later use with dense retriever
document_store = ElasticsearchDocumentStore(return_embedding=True)

